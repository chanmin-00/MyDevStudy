실행 중인 프로그램은 모두 메모리에 저장되어 있으며, CPU는 메모리에 저장된 정보를 읽고, 쓰고, 실행한다.

# RAM

컴퓨터의 메인 메모리 역할을 하는 하드웨어는 RAM과 ROM이 있지만, 일반적으로 ‘메인 메모리’라고 하면 **RAM**을 의미한다.

RAM은 전원을 끄면 저장된 데이터와 명령어가 사라지는 휘발성 저장장치(Volatile Memory) 이며, CPU가 실제로 **실행할 프로그램과 데이터를 임시로 저장하는 역할**을 한다.

이에 반해 보조기억장치(HDD, SSD 등)는 전원이 꺼져도 데이터가 남는 비휘발성 저장장치(Non-volatile Memory) 로, 프로그램이나 데이터를 장기 보관하는 용도로 사용된다.

### CPU와 RAM

CPU는 프로그램을 실행하기 위해 보조기억장치에 저장된 프로그램을 RAM으로 복사한 후 실행한다. 이 과정에서 RAM의 용량은 컴퓨터 성능에 직접적인 영향을 준다.

- **RAM 용량이 작을 경우**
    
    → 실행할 때마다 보조기억장치에서 프로그램을 불러와야 하므로 실행 시간이 길어진다.
    
- **RAM 용량이 클 경우**
    
    → 더 많은 데이터를 한 번에 메모리에 저장할 수 있어, 여러 프로그램을 동시에 빠르게 실행할 수 있다.
    

하지만 RAM 용량이 크다고 해서 **성능이 무조건 비례해 향상되는 것은 아니다.**

### RAM의 메모리 접근 방식

RAM은 **Random Access Memory**, 즉 ‘임의 접근 메모리’라는 뜻을 가진다. 임의 접근이란 데이터의 저장 위치에 상관없이 즉시 접근할 수 있는 방식을 의미한다.

예를 들어, 100번지에 있는 데이터에 접근하려면 1번지부터 차례로 읽을 필요 없이 바로 100번지로 이동할 수 있다. 이처럼 RAM은 **1번지나 1000번지든 접근 속도가 동일**하다.

이에 반해 **순차 접근(Sequential Access)** 은 데이터가 저장된 순서대로 접근해야 한다. 따라서 접근할 위치가 멀수록 시간이 오래 걸린다.

## RAM의 주요 종류

### 1. DRAM

DRAM은 Dynamic(동적)이라는 이름처럼 시간이 지나면 저장된 데이터가 점차 사라지는 특징을 가진다. 이를 방지하기 위해 주기적으로 **데이터를 재활성화(다시 저장)**해야 한다.

그래서 시간이 지나면 데이터가 소멸되므로 주기적 갱신이 필요하다. 이 단점에도 불구하고 소비 전력이 낮고, 가격이 저렴하며, 집적도가 높다. 그래서 대용량 메모리 설계에 유리해 대부분의 컴퓨터에서 메인 메모리로 사용한다.

### 2. SRAM

SRAM은 Static(정적)이라는 이름처럼 저장된 데이터가 시간이 지나도 변하지 않는다. 단, 전원이 꺼지면 데이터는 소실되므로 비휘발성 메모리는 아니다.

DRAM과 비교해 속도는 빠르지만, 소비 전력이 크고 가격도 비싸고 집적도가 낮아 대용량으로 만들 필요는 없지만 속도가 빨라야 하는 장치에 적합하다.CPU 캐시 메모리 등 속도가 중요한 부분에 사용한다.

### 3. SDRAM **(Synchronous DRAM)**

SDRAM은 기존 DRAM에 클럭 신호와의 동기화 기능을 추가한 형태이다. ‘Synchronous’라는 이름 그대로, CPU의 클럭에 맞춰 데이터를 주고받는다.

이 방식은 CPU의 동작 주기와 정확히 맞물려 데이터 처리 효율을 높였다는 점에서 기존 DRAM보다 훨씬 빠르고 안정적이다.

### 4. DDR SDRAM (**Double Data Rate SDRAM)**

DDR SDRAM은 SDRAM을 기반으로 데이터 전송 효율을 두 배로 높인 메모리이다. ‘Double Data Rate’라는 이름처럼 한 클럭 주기 동안 두 번의 데이터 전송이 가능하다.

대역폭은 데이터가 이동하는 통로의 너비를 의미하며, DDR은 이 대역폭을 2배 확장한 것이다. 

DDR → DDR2 → DDR3 → DDR4 → DDR5로 발전하면서 세대마다 속도와 효율이 향상되었다. 예를 들어 DDR4 SDRAM은 SDR SDRAM보다 약 16배 넓은 대역폭을 제공하며, 현재 대부분의 PC와 노트북에서 표준으로 사용된다.

<img width="471" height="183" alt="image" src="https://github.com/user-attachments/assets/f399c46b-542f-452b-aecf-0e6f7c28f963" />

# 메모리에 바이트를 밀어 넣는 순서 - 빅엔디안과 리틀 엔디안

현대의 메모리는 대부분 데이터를 바이트 단위로 저장하고 관리하지만, CPU는 데이터를 4바이트 혹은 8바이트 단위(워드 단위) 로 처리한다. 따라서 여러 바이트로 구성된 데이터를 메모리에 저장할 때는, 이 데이터를 여러 개의 주소 공간에 나누어 저장하게 된다.

예를 들어, 메모리는 한 주소에 1바이트씩 저장하므로 4바이트 데이터는 4개의 주소, 8바이트 데이터는 8개의 주소를 차지한다.

16진수 값 1A2B3C4D를 예로 들면, 이 데이터는 8비트씩 나누어 총 4개의 주소 공간에 걸쳐 저장된다.

이때 **연속된 바이트를 어떤 순서로 저장하느냐**에 따라 **빅엔디안(Big Endian)** 과 **리틀엔디안(Little Endian)** 두 가지 방식으로 나뉜다.

### 빅엔디안

빅엔디안은 **낮은 번지의 주소에 상위 바이트**부터 저장하는 방식이다. 여기서 상위 바이트란, 숫자 크기에 가장 큰 영향을 미치는 바이트를 뜻한다. 예를 들어, 1A2B3C4D 중 **가장 큰 값은 1A(상위 바이트)** 이다.

이 값을 메모리의 a+2번지부터 빅엔디안 방식으로 저장한다고 하면, 낮은 주소부터 1A, 2B, 3C, 4D 순서로 저장된다.

<img width="418" height="212" alt="image" src="https://github.com/user-attachments/assets/5498db55-ba9f-46b4-9753-8a208d21e304" />

즉, 우리가 평소 10진수를 읽는 순서(큰 자리수 → 작은 자리수)와 동일한 형태로 저장된다. 이 때문에 **빅엔디안은 사람이 읽거나 디버깅할 때 이해하기 쉽다.**

### 리틀엔디안

리틀엔디안은 반대로, **낮은 번지의 주소에 하위 바이트**부터 저장하는 방식이다. 하위 바이트란, 숫자 크기에 가장 작은 영향을 미치는 바이트를 의미한다. 1A2B3C4D의 최하위 바이트는 4D이다.

이 값을 a+2번지부터 리틀엔디안 방식으로 저장하면, 낮은 주소부터 4D, 3C, 2B, 1A 순서로 저장된다.

<img width="436" height="209" alt="image" src="https://github.com/user-attachments/assets/9e73e84a-a2ad-4baf-ba89-348c17ae2c56" />

이 방식은 사람이 직접 메모리 내용을 읽을 때는 다소 불편하지만, **수치 계산 시에는 훨씬 효율적이다.**

예를 들어, 10진수에서 123 + 456을 계산할 때 일의 자리(3과 6) 부터 계산하듯, 리틀엔디안은 작은 단위부터 저장되어 있으므로 계산이나 자리올림(carry) 처리를 시작하기가 용이하다

### **MSB와 LSB 개념**

엔디안 방식을 이해하기 위해서는 MSB(Most Significant Bit) 와 LSB(Least Significant Bit) 개념을 알아둘 필요가 있다.

- **MSB (Most Significant Bit)**
    - 숫자의 크기에 가장 큰 영향을 미치는 비트로, 가장 왼쪽 비트이다.
    - 예: 10진수 123 → MSB는 1
- **LSB (Least Significant Bit)**
    - 숫자의 크기에 가장 작은 영향을 미치는 비트로, 가장 오른쪽 비트이다.
    - 예: 10진수 123 → LSB는 3

빅엔디안은 MSB(큰 단위) 부터 저장해 나가는 방식이고, 리틀엔디안은 LSB(작은 단위) 부터 저장해 나가는 방식이다.

### **예시: 실수 데이터의 저장**

10진수 소수인 107.6640625는 16진수 42d75400으로 표현되며, 빅엔디안 방식으로 16진수를 표현하면 

실행 결과는 42d75400이다.

리틀엔디안 방식으로 표현하게 되면 0054d742로 표시가 된다. 리틀 엔디안 방식으로 저장된 10진수 소수 107.66406225는 메모리 내에 0054d742로 저장된다.

# 캐시 메모리

CPU가 연산을 수행할 때, **메모리에 접근하는 속도**는 **레지스터에 접근하는 속도보다 훨씬 느리다.** 아무리 CPU의 연산 속도가 빨라도, 메모리 접근 속도가 그에 따라가지 못하면 CPU의 성능은 제대로 발휘될 수 없다.

이처럼 CPU와 메모리 간 속도 차이로 인해 발생하는 병목 현상을 줄이기 위해 등장한 것이 바로 **캐시 메모리(Cache Memory)** 이다.

### 캐시 메모리의 개념

캐시 메모리는 **CPU의 연산 속도와 메모리 접근 속도의 차이를 줄이기 위해 만들어진 고속 저장장치**이다. CPU와 메모리 사이에 위치하며, **CPU가 자주 사용할 데이터나 명령어를 미리 저장해두는 역할**을 한다.

메모리에 직접 접근하면 시간이 오래 걸리기 때문에, CPU는 먼저 캐시 메모리에서 필요한 데이터를 찾아본다. 만약 캐시에 해당 데이터가 있으면 이를 바로 사용하고, 없으면 메인 메모리(RAM)에서 데이터를 가져와 캐시에 저장한 뒤 사용한다.

이 과정을 통해 CPU는 훨씬 빠르게 데이터를 처리할 수 있다.

캐시 메모리는 일반적으로 SRAM(Static RAM) 기반으로 만들어진다. SRAM은 DRAM보다 빠르지만, 비싸고 집적도가 낮기 때문에 용량은 작고 속도는 매우 빠른 메모리로 적합하다.

<img width="486" height="183" alt="image" src="https://github.com/user-attachments/assets/2bc00ba5-9742-4361-9b5a-baf5fb86cda1" />

### **캐시 메모리의 계층 구조 (L1, L2, L3)**

컴퓨터 내부에는 여러 단계의 캐시 메모리가 존재한다. 이는 CPU와의 물리적 거리 및 접근 속도에 따라 L1, L2, L3 캐시로 구분된다.

- **L1 캐시 (Level 1 Cache)**
    - CPU 코어에 가장 가까이 위치한 캐시로, 속도가 가장 빠르다.
    - 일반적으로 코어 내부에 존재한다.
- **L2 캐시 (Level 2 Cache)**
    - L1보다 조금 느리지만 용량이 더 크다.
    - 대부분의 경우 코어 내부에 존재한다.
- **L3 캐시 (Level 3 Cache)**
    - 여러 코어가 공유하는 캐시로, L2보다 크고 느리다.
    - 보통 코어 외부에 위치한다.

<img width="475" height="149" alt="image" src="https://github.com/user-attachments/assets/90579fd5-b269-44a4-b72a-663a254a29ab" />

멀티 코어 프로세서의 경우 일반적으로 L1 캐시 메모리와 L2 캐시 메모리는 코어마다 고유한 캐시 메모리로 할당되고 L3 캐시는 여러 종류가 공유하는 형태로 구현된다.

CPU가 어떤 데이터에 접근하려고 할 때는 다음 순서로 캐시를 탐색한다.

1. L1 캐시에서 먼저 데이터를 찾는다.
2. L1에 없으면 L2 캐시를 확인한다.
3. L2에도 없으면 L3 캐시를 검색한다.
4. L3에도 없을 경우, 메인 메모리(RAM) 에서 데이터를 가져온다.

이러한 과정을 통해 CPU는 가장 가까운 저장장치부터 검색하며 불필요한 메모리 접근을 최소화한다.

### **L1 캐시의 세분화**

CPU 코어와 가장 가까운 L1 캐시는 더욱 세밀하게 구분되기도 한다. L1 캐시 내부는 보통 다음 두 가지로 나뉜다.

- L1I 캐시 (Instruction Cache) : 명령어를 저장하는 캐시
- L1D 캐시 (Data Cache) : 데이터를 저장하는 캐시

이렇게 명령어와 데이터를 분리해 저장하는 구조를 **분리형 캐시**라고 부른다. 분리형 캐시는 동시에 명령어와 데이터를 병렬로 처리할 수 있어 CPU의 효율성을 극대화한다.

<img width="483" height="183" alt="image" src="https://github.com/user-attachments/assets/7d22130a-7ce8-4aa1-a1e9-c57531924acf" />

## 캐시 히트와 캐시 미스

캐시 메모리는 CPU의 속도와 메모리의 속도 차이를 줄이기 위해 만들어졌지만, 그 용량이 메모리에 비해 매우 작기 때문에 **메모리의 모든 데이터를 캐시에 저장할 수는 없다.**

따라서 캐시 메모리는 CPU가 **곧 사용할 가능성이 높은 데이터**만을 선별해 임시로 저장한다. 즉, 캐시는 전체 메모리의 일부를 복사해두는 일종의 예측을 기반으로 한 저장 공간이다.

보조기억장치는 전원이 꺼져도 데이터를 영구적으로 보관하고, 메모리(RAM) 는 현재 실행 중인 데이터를 저장하며, **캐시 메모리**는 CPU가 **가까운 시점에 사용할 법한 데이터**를 미리 저장한다.

### 캐시 히트

캐시 메모리에 저장된 데이터가 CPU가 실제로 필요로 하는 데이터와 일치할 때, 즉 CPU가 요청한 데이터가 캐시 안에 존재할 때를 캐시 히트(Cache Hit) 라고 한다.

이 경우 CPU는 메인 메모리에 접근할 필요가 없으므로 매우 빠르게 데이터에 접근할 수 있다. 

### 캐시 미스

반대로, 캐시 메모리가 예측하여 저장한 데이터가 CPU가 실제로 필요로 하는 데이터와 다를 때를 캐시 미스라고 한다.

이 경우 CPU는 **캐시에 데이터가 없으므로 메모리(RAM)** 에 직접 접근해야 한다. 이 과정은 캐시 히트보다 훨씬 느리며, CPU의 처리 효율이 떨어지게 된다.

즉, 캐시 미스가 많을수록 캐시 메모리의 장점을 살릴 수 없고, 전체적인 CPU 성능이 저하된다.

<img width="531" height="200" alt="image" src="https://github.com/user-attachments/assets/0f822ca3-3414-4630-9ea4-ce092336dbe5" />

### 캐시 적중률

캐시가 히트되는 비율을 캐시 적중률이라고 한다. 일반적으로 범용 컴퓨터의 캐시 적중률은 85%~95% 수준이며, 이 수치가 높을수록 CPU는 메모리에 직접 접근할 필요가 줄어들어 전체 시스템의 처리 속도가 크게 향상된다.

## 참조 지역성의 원리

캐시 메모리의 가장 큰 목적은 CPU의 메모리 접근 속도를 향상시키는 것이다. 하지만 캐시의 용량은 제한되어 있기 때문에, 어떤 데이터를 캐시에 저장할지 정확하게 예측하는 것이 매우 중요하다.

CPU가 실제로 사용할 가능성이 높은 데이터를 효율적으로 캐시에 저장해야 캐시 적중률을 높일 수 있으며, 이 예측의 근거가 되는 개념이 바로 참조 지역성의 원리이다.

참조 지역성은 CPU가 메모리에 접근할 때 보이는 주된 패턴이나 경향성을 말한다. 즉, 프로그램은 무작위로 메모리에 접근하지 않고, **특정 영역이나 근처의 데이터를 집중적으로 반복해서 참조하는 경향**이 있다는 것이다.

이 원리는 두 가지 주요 형태로 나뉜다

- 시간 지역성
- 공간 지역성

### 시간 지역성

**시간 지역성이란, CPU가 최근에 접근했던 메모리 공간을 다시 접근하려는 경향을 의미한다.**

예를 들어, 프로그램에서 변수를 사용할 때 그 변수는 한 번만 사용되고 끝나는 경우보다, 여러 번 반복적으로 참조되는 경우가 훨씬 많다. 즉, 최근에 읽거나 쓴 데이터는 가까운 미래에도 다시 사용될 가능성이 높다.

### 공간 지역성

공간 지역성은 CPU가 한 번 접근한 메모리 주소 주변의 인접한 주소들도 곧 접근하려는 경향을 의미한다.

대표적인 예로 배열이 있다. 배열은 메모리 상에서 연속된 공간에 저장되므로, 하나의 요소를 참조할 때 그 다음 요소도 곧 접근할 가능성이 크다.

<img width="349" height="179" alt="image" src="https://github.com/user-attachments/assets/b291c896-c34d-4e7f-b965-768d9f6553df" />

## 캐시 메모리의 쓰기 정책과 일관성

CPU가 캐시 메모리에 데이터를 쓸 때는 캐시 메모리의 데이터와 메인 메모리의 데이터가 서로 일관성을 유지해야 한다. 즉, CPU가 데이터를 변경할 경우, 캐시와 메모리의 값이 서로 달라지는 불일치상황이 발생하지 않도록 관리해야 한다.

예를 들어, 현재 메모리의 1000번지에 값 200이 저장되어 있고, 이 값이 캐시 메모리에도 복사되어 있다고 하자. 

이 상태에서 CPU가 1000번지의 값을 200에서 300으로 변경하려 할 때, 단순히 메모리의 값을 300으로 바꾸면 문제가 생긴다. 왜냐하면 CPU는 여전히 캐시 메모리에 저장된 오래된 값(200) 을 참조하고 있기 때문이다.

결국 CPU가 1000번지의 값을 출력하면, 메모리의 최신 값(300)이 아닌 캐시에 남아 있는 이전 값(200)이 출력되는 불일치가 발생한다.

<img width="252" height="245" alt="image" src="https://github.com/user-attachments/assets/e3141d45-b45d-4d6e-a361-b000387964a1" />

이런 문제를 방지하기 위해 캐시 메모리는 두 가지 주요 쓰기 정책을 사용한다.

### **1. 즉시 쓰기 (Write-Through)**

즉시 쓰기는 데이터를 캐시 메모리와 메인 메모리에 동시에 기록하는 방식이다. 즉, CPU가 데이터를 수정하면 캐시뿐 아니라 메모리에도 즉시 반영된다.

이 방식은 항상 메모리가 최신 상태로 유지되므로, 캐시와 메모리 간의 불일치가 발생하지 않는다.

하지만 데이터를 쓸 때마다 메모리에도 접근해야 하므로 **쓰기 속도가 느리다.** 또한 버스 사용량이 늘어나고, 캐시의 성능 이점이 일부 사라진다.

즉시 쓰기 방식은 일관성은 높지만 효율이 떨어지는 구조이다. 메모리 접근을 줄이기 위해 캐시를 두었는데, 매번 메모리에 접근해야 하기 때문이다.

<img width="350" height="181" alt="image" src="https://github.com/user-attachments/assets/fb9b6f5b-696c-4dc8-a891-afec35cc636d" />

### **2. 지연 쓰기 (Write-Back)**

지연 쓰기는 CPU가 데이터를 수정할 때 우선 캐시 메모리에만 반영하고, 나중에 변경된 데이터를 한 번에 메모리에 반영하는 방식이다.

이 방식은 캐시에서 자주 수정되는 데이터를 여러 번 메모리에 기록하지 않아도 되므로, 쓰기 효율이 매우 높고 전체 성능이 향상된다.

이 방식은 메모리 접근 횟수가 줄어들어 성능이 향상된다. 하지만 메모리와 캐시 사이의 데이터가 일시적으로 불일치 상태가 될 수 있다.

따라서 지연 쓰기 방식은 효율은 높지만 일관성 관리가 어려운 방식이다.

<img width="351" height="183" alt="image" src="https://github.com/user-attachments/assets/8524f06a-f918-4e92-97a9-4ba111b568e9" />

캐시 메모리의 일관성 문제는 단일 코어뿐만 아니라 멀티코어 환경에서 더욱 복잡해진다. 멀티코어 프로세서에서는 각 코어가 자신만의 캐시 메모리를 갖고 있으며, 서로 다른 코어가 같은 메모리 주소를 다룰 수도 있다.

이때 한 코어가 데이터를 변경하더라도, 다른 코어의 캐시에는 여전히 이전 데이터가 남아 있을 수 있다.

즉, 캐시 메모리는 CPU가 접근하는 속도를 높이는 장치이지만, 데이터를 다룰 때는 메모리와의 일관성을 반드시 고려해야 한다.
