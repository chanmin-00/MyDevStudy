## 비트와 바이트

---

- 컴퓨터는 0과 1로 이루어진 이진수만 이해할 수 있으며, 이 중 0과 1 하나하나를 표현하는 가장 작은 정보 단위를 비트(bit)
- 1비트는 두 가지 상태(0 또는 1)를 표현할 수 있으며, 2비트는 4가지, 3비트는 8가지 상태를 표현할 수 있음
- 하지만 비트는 너무 작은 단위이기 때문에, 실행되는 프로그램은 보통 수십만~수백만 비트로 구성됨.
    
    따라서 프로그램의 크기를 표현할 때는 비트보다 큰 단위인 바이트(Byte), 킬로바이트(KB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB) 등을 사용
    
    ![image](https://github.com/user-attachments/assets/5533b442-8318-4d4a-a1eb-cf7fcf92af21)

    
- 1바이트는 8비트로 구성되어 있으며, 1바이트로는 총 256가지(2⁸)의 정보를 표현할 수 있음
- 이처럼 비트, 바이트, KB, MB, GB, TB 등은 프로그램의 크기를 나타낼 때 사용하는 정보 단위로, 프로그램 관점의 단위
    
    ```bash
    1바이트 = 8비트
    1KB = 1,000바이트
    1MB = 1,000KB
    1GB = 1,000MB
    1TB = 1,000GB
    
    (과거에는 1KB = 1,024바이트로 사용되기도 함)
    현재는 보통 1,000 단위로 표기하는 것이 일반적
    ```
    

### CPU 관점의 정보 단위: 워드(Word)

- 한편, CPU는 프로그램을 한 번에 모두 처리하지 않고, 워드(Word) 단위로 데이터를 처리
- 워드란 CPU가 한 번에 처리할 수 있는 데이터의 크기를 의미하며, CPU마다 워드의 크기는 다를 수 있음.
    - 예: 16비트를 한 번에 처리할 수 있는 CPU의 워드 크기는 16비트
    - 현재 대부분의 컴퓨터는 워드 크기가 32비트 또는 64비트
- 즉, 프로그램의 전체 크기가 2GB라고 해도 CPU는 이를 한 번에 처리하는 것이 아니라, 워드 단위로 나누어 순차적으로 처리

## 데이터 - 0과 1로 숫자 표현하기

---

### 2진법 (Binary)

- 컴퓨터는 사람이 사용하는 10진법(0~9)이 아닌 2진법(0과 1)만을 이해
- 2진법도 10진법처럼 자리 올림 규칙이 있으며, 숫자 1을 넘어가는 시점에 자릿수가 올라감.
- 단 두 가지 숫자인 0과 1만으로도 모든 수를 표현할 수 있음.
- 2진수로 표현된 수는 숫자 뒤에 아래첨자 (2) 또는 앞에 `0b`를 붙여 나타냄.
    - 예: `1101₂` 또는 `0b1101`
- 2진수는 길어지기 쉬우므로, 4자리 2진수 = 1자리 16진수로 축약해 표현하기도 함.

![image](https://github.com/user-attachments/assets/7c901a17-6483-4d74-a3d3-e3fd131e7b07)


### 16진법 (Hexadecimal)

- 컴퓨터에서 정보를 표현할 때는 2진수뿐만 아니라 16진수도 자주 사용됨.
- 16진법은 15를 넘어가면 자리 올림이 발생하는 숫자 표현 방식
- 10진수 0~15를 16진수로 나타내면 다음과 같음.
    - `0 ~ 9`, `A ~ F` (예: 10 → A, 11 → B, ..., 15 → F)
- 16진수는 숫자 뒤에 아래첨자 (16) 또는 앞에 `0x`를 붙여 표현
    - 예: `0x1A`, `2F₁₆`
- 16진수는 소스 코드, MAC 주소, IPv6 주소 등에서도 널리 사용
    - 예시:
        - MAC 주소: `A1:C2:E3:A5:C6:E7`
        - IPv6 주소: `2001:0a1b:1234:0000:0000:abcd:1234:ff02`

![image](https://github.com/user-attachments/assets/b602f7f9-145b-4d4a-be94-cfbf206e58cb)


### 실수(소수)의 2진 표현과 오차

- 실수는 정확하게 표현되지 않을 수 있음
- 컴퓨터는 2진법만 이해하므로, 10진수 소수를 정확하게 표현하지 못할 수 있음.
- 실수는 대부분 근사값으로 저장되며, 이로 인해 오차가 발생할 수 있음.

```python
a = 0.1
b = 0.2
c = 0.3
if a + b == c:
    print("Equal")
else:
    print("Not Equal")

출력 결과: Not Equal
이유: 0.1, 0.2, 0.3은 정확한 2진수 표현이 불가능하기 때문입니다.
```

### 실수 표현의 오차 이유

- 컴퓨터는 실수를 부동 소수점(Floating Point) 방식으로 저장
- 이는 소수점 위치가 고정되지 않고, 가수 × 지수의 형태로 표현되는 방식
- 예시:
    - 10진수 `123,123`은 `1.23123 × 10⁵`처럼 표현 가능
    - 이를 일반화하면: `실수 = 가수 × 밑^지수`
    - 10진수: `m × 10ⁿ`
    - 2진수: `m × 2ⁿ`
- 10진수 소수는 대부분 2진수로 정확하게 표현할 수 없음
    - 예: 1/3 → 10진수: `0.333...`, 2진수: `무한소수`
    - 예: `0.1`도 정확히 2진수로 표현 불가 → 근사값 저장
- 무한 소수를 정확히 저장하려면 무한한 비트가 필요하지만, 저장 공간에는 한계가 있음
- 일부 값은 생략되어 저장되며, 이로 인해 오차 발생

### IEEE 754 표준

- 대부분의 컴퓨터는 IEEE 754라는 부동 소수점 표준에 따라 실수를 저장.
- 실수는 다음과 같이 세 부분으로 나누어 저장:
    - 부호(Sign): 1비트
    - 지수(Exponent): 8비트
    - 가수(Fraction/Mantissa): 23비트
    - 총 32비트 (float 기준)

![image](https://github.com/user-attachments/assets/14b6535d-bd8c-4fb3-8d61-102b6af10d38)


```bash
10진수 107.6640625 → 2진수: 1101011.1010101
과학적 표기: 1.1010111010101 × 2⁶

IEEE 754 표현:
    - 부호 비트: 0 (양수)
    - 지수: 6 + 127 = 133 → 10000101
    - 가수: 10101110101010000000000

최종 저장 형태:
    
    0 10000101 10101110101010000000000
```

### 바이어스(Bias)

- 지수를 저장할 때는 바이어스(Bias)를 더해서 저장
- 바이어스 값 = `2$^{(k-1)}$ - 1` (k = 지수 비트 수)
    - 예: 지수 비트가 8비트 → 바이어스 = `2⁷ - 1 = 127`

> 부동소수점 표현에서는 1.01 × 2⁻³처럼 음의 지수가 필요한 경우가 많다. 하지만 지수를 저장할 때 사용하는 비트는 부호가 없는 이진수(양수)이다. 이는 부호 비트를 따로 두지 않기 때문인데, IEEE 754 표준에서는 지수에 부호를 표시하지 않고, 단순한 이진수로만 저장한다.
> 
> 
> 이렇게 하면 하드웨어적으로 처리 속도가 빠르고 구조가 단순해지기 때문에 효율적이다.
> 
> 하지만 이 방식은 음수 지수를 직접 표현할 수 없다는 한계가 있습니다. 이를 해결하기 위해, 지수에 '바이어스(Bias)'를 더해 양수로 변환한 뒤 저장한다.
> 
> 예를 들어 지수 비트가 8비트라면 바이어스는 2⁷ - 1 = 127이 된다. 실제 지수가 -3이면 -3 + 127 = 124로 저장하고, 읽을 때 다시 124 - 127 = -3으로 복원하는 방식이다.
> 

## 데이터 - 0과 1로 문자 표현하기

---

- 컴퓨터가 이해할 수 있는 문자들의 모음을 문자 집합(Character Set)이라고 함
- 이 문자 집합에 포함된 문자들을 컴퓨터가 이해할 수 있는 0과 1로 이루어진 코드로 바꾸는 과정을 문자 인코딩(Encoding)
- 반대로, 0과 1로 표현된 데이터를 사람이 이해할 수 있는 문자로 바꾸는 과정은 문자 디코딩(Decoding)
- 동일한 문자 집합이라 하더라도, 여러 가지 문자 인코딩 방식이 존재할 수 있음.

### 문자 깨짐의 원인

- 웹사이트에서 문자가 깨져서 보이는 현상은
    - 웹사이트가 특정 인코딩 방식을 지원하지 않거나,
    - 인코딩된 문자를 올바른 방식으로 디코딩하지 못할 때 발생

### 아스키(ASCII) 문자 집합

- 가장 기본적인 문자 집합 중 하나는 아스키(ASCII)
- 아스키는 초기 컴퓨터에서 사용되던 문자 집합으로, 영어 알파벳 대소문자, 숫자(0~9), 일부 특수 문자들만 포함
- 아스키 문자를 표현하기 위해 8비트 중 7비트를 사용하며, 나머지 1비트는 패리티 비트(parity bit)로, 오류 검출에 사용
- 7비트로 표현할 수 있는 조합은 총 128개 (2⁷)
    - 예 : 아스키 문자 `A` → 10진수 `65` → 2진수 `1000001`
    - 이렇게 각 문자에 대응되는 고유 숫자값을 아스키 코드라고 하며, 이를 2진수로 변환하면 컴퓨터가 처리할 수 있는 형태로 인코딩 됨

![image](https://github.com/user-attachments/assets/c0ab6234-b175-42c6-9a93-1876e5c70155)


> 문자 인코딩에서 각 문자에 부여된 고유 숫자값을 코드 포인트(Code Point)라고 부른다.
> 

- 아스키 코드는 영어와 일부 특수 문자만 표현할 수 있고, 한글, 한자, 기타 세계 여러 언어의 문자는 표현 불가

### **한글 인코딩 방식: EUC-KR과 유니코드**

- 아스키(ASCII) 코드는 영문과 숫자 등 기본 문자만 표현할 수 있어 한글 표기가 불가능
- 이를 보완하기 위해 등장한 것이 EUC-KR 인코딩 방식
- EUC-KR은 KS X 1001, KS X 1003 문자 집합을 기반으로 하며,
    - 영문(아스키 문자): 1바이트
    - 한글 글자 하나: 2바이트 (16비트, 네 자리 16진수)
    - 로 표현
- 예를 들어,
    - `‘한’ → 0xC7D1`
    - `‘글’ → 0xB1DB` 처럼 인코딩
- 하지만 EUC-KR로는 약 2,350개의 한글 글자만 표현할 수 있어, ‘똠’, ‘쀍’ 같은 글자는 표현 불가
- 이 한계를 극복하기 위해 등장한 것이 유니코드(Unicode)
- 유니코드는 한글을 포함해 전 세계 대부분의 문자, 기호, 이모지까지 표현할 수 있는 통일된 문자 집합
    - `‘한’ → 0xD55C`
    - `‘글’ → 0xAE00 (유니코드 값)`
        
      
        

> 유니코드 덕분에 언어마다 다른 문자 집합을 이해할 필요 없이, 하나의 시스템으로 전 세계 문자를 처리할 수 있게 되었다. 현재 가장 널리 쓰이는 문자 인코딩 표준이다.
> 

### **유니코드 인코딩 방식 (UTF-8, UTF-16, UTF-32)**

- 아스키(ASCII)나 EUC-KR은 문자에 부여된 값을 그대로 인코딩 값으로 사용
- 하지만 유니코드는 문자에 부여된 값을 다양한 방식으로 인코딩
- 유니코드 인코딩 방식에는 UTF-8, UTF-16, UTF-32가 있으며, 이들은 유니코드 문자에 부여된 값을 인코딩하는 방식
- 모두 가변 길이 인코딩 방식입니다. 인코딩 결과의 길이가 일정하지 않다는 의미
    
    ![image](https://github.com/user-attachments/assets/854e4608-5c28-4f28-81ab-972c72ec26dc)

    

### Base64 인코딩

- Base64는 단순한 문자 인코딩을 넘어, 이진 데이터를 문자 형태로 안전하게 표현할 수 있는 방식
- 특히 이미지나 바이너리 파일을 텍스트 기반 환경(e.g. 이메일, JSON 등)에서 전송할 때 자주 사용됨
- Base64는 64진법 기반 인코딩
- 64개의 고정 문자 집합(A–Z, a–z, 0–9, +, /)을 사용해 데이터를 표현
    - 64진수 → 6비트 (2⁶)
    - Base64는 데이터를 6비트씩 나누어 각각 하나의 문자로 변환
- 보통 24비트(3바이트) 단위로 묶어 → 6비트 × 4조각 → 문자 4개로 인코딩

```bash
예시: 문자열 abc 인코딩

1.	a, b, c의 아스키 값: 97, 98, 99

2.	이진수로 변환:

		a → 01100001  
		b → 01100010  
		c → 01100011  
		
3.	총 24비트 → 6비트씩 나누면:

		011000 010110 001001 100011

4.	각 조각을 Base64 문자로 매핑 → YWJj
```

![image](https://github.com/user-attachments/assets/b0426d7f-6920-41e5-bfaa-378654ccb27b)


### **패딩(Padding)의 개념**

- 인코딩할 데이터가 3바이트의 배수가 아닐 경우, 남는 비트를 0으로 채우고, 결과 끝에 = 기호로 패딩 처리

```bash
예: 'ab'는 총 16비트 → 6비트 × 3조각 + 2비트 부족

부족한 비트를 0으로 채움 → Base64 결과: YWI=
```

![image](https://github.com/user-attachments/assets/10fc12e9-f18f-4cf6-80c8-612cbe2df1f1)


## 명령어

---

### **명령어 구성: 연산 코드(Opcode)와 오퍼랜드(Operand)**

- 명령어는 수행할 동작(연산 코드) + 수행할 대상(오퍼랜드)로 구성
- 오퍼랜드는 실제 데이터(값) 또는 데이터가 저장된 위치(주소)일 수 있음
- 연산 코드: 연산 종류를 지정
- 오퍼랜드: 연산에 사용할 데이터나 주소

![image](https://github.com/user-attachments/assets/1a8d8be3-d343-48c3-87c9-4ddb1a1c6e48)


```bash
예시:

- 더해라 100과 120을

- 빼라 메모리 32번지 값과 33번지 값을

- 저장해라 10을 메모리 128번지에
```

- 오퍼랜드는 주로 데이터의 주소(예: 메모리 주소)를 가리키기 때문에 주소 필드라고도 함

```bash
예를 들어 더해라, 100번지 값에, 10을이라는 명령어가 있다면:

CPU는 먼저 명령어를 인출

다음으로 오퍼랜드가 지정한 메모리 주소에 접근해 실제 데이터를 가져옴
```

![image](https://github.com/user-attachments/assets/c2b42a34-1c88-4bab-aebb-3fe324347bdd)


- 오퍼랜드가 직접 값이 아닌 간접적으로 데이터를 가리키는 역할을 하면 CPU는 한 번 더 메모리에 접근해야 함

### 연산코드의 유형

- CPU가 공통적으로 이해하는 대표적인 연산코드의 유형에는 데이터 전송, 산술/논리 연산, 제어 흐름 변경, 입출력 제어가 존재

| **연산 코드** | **설명** |
| --- | --- |
| MOVE | 데이터를 옮겨라 |
| STORE | 데이터를 메모리에 저장하라 |
| LOAD(FETCH) | 데이터를 메모리에서 CPU로 가져와라 |
| PUSH / POP | 스택에 저장 / 스택에서 꺼내라 |

| **연산 코드** | **설명** |
| --- | --- |
| ADD/SUBTRACT/… | 덧셈, 뺄셈, 곱셈, 나눗셈 수행 |
| INCREMENT / DECREMENT | 1 더하기 / 빼기 |
| AND / OR / NOT | 논리 연산 수행 |
| COMPARE | 두 값을 비교해 TRUE/FALSE 결과 |

| JUMP | 특정 주소로 실행 순서 이동 |
| --- | --- |
| CONDITIONAL JUMP | 조건 만족 시 특정 주소로 이동 |
| HALT | 프로그램 실행 멈춤 |
| CALL / RETURN | 서브루틴 호출 및 복귀 |

| READ / WRITE | 입출력 장치로부터 읽기 / 쓰기 |
| --- | --- |
| START IO | 입출력 장치 작동 시작 |
| TEST IO | 입출력 장치 상태 확인 |

### **기계어와 어셈블리어**

- 기계어(Machine Code): CPU가 직접 이해할 수 있는 0과 1로 된 명령어
    
    
- 어셈블리어(Assembly Language): 기계어를 사람이 이해하기 쉽게 바꾼 저수준 언어
    - 어셈블리어를 보면 CPU가 이해할 수 있는 명령어의 종류와 동작을 파악 가능

```bash
기계어 0101 0101 → 어셈블리어 push rbp
기계어 1100 0011 → 어셈블리어 ret
```

### **소스코드 → 어셈블리 → 기계어**

```c
int square(int num) {
  return num * num;
}
```

- 이 코드는 결국 CPU가 이해할 수 있는 기계어로 번역되어 실행됨
- 이때의 중간 번역 형태가 어셈블리어이며, 연산 코드와 오퍼랜드로 구성됨
- 유의할 점은 구체적인 연산코드의 종류나 레지스터의 이름, 명령어의 생김새는 CPU마다 다를 수 있다는 점

### **CPU 구조별 어셈블리어 차이**

- CISC 기반 CPU (ex. 인텔)

```nasm
push rbp
mov rbp, rsp
mov DWORD PTR [rbp-4], edi
mov eax, DWORD PTR [rbp-4]
imul eax, eax
pop rbp
ret
```

- RISC 기반 CPU (ex. ARM 등)

```nasm
push rbp
mov rbp, rsp
mov DWORD PTR [rbp-4], edi
mov eax, DWORD PTR [rbp-4]
imul eax, eax
pop rbp
ret
```

- CPU 아키텍처에 따라 어셈블리 명령어가 달라지므로, 특정 플랫폼에서 실행될 프로그램은 CPU 의존적인 코드로 만들지 않는 것이 중요

### 명령어 사이클

- 메모리에는 프로그램이 저장되어 있고, 이 프로그램은 여러 명령어들의 집합으로 구성되어 있음
- CPU는 이 명령어들을 메모리에서 하나씩 인출(fetch) → 실행(execute)하는 과정을 반복하여 전체 프로그램을 수행함
- 이렇게 CPU가 명령어를 처리하는 일련의 흐름을 명령어 사이클이라 부름. 프로그램 내 각 명령어가 이 사이클에 따라 반복적으로 실행됨

### 명령어 사이클의 단계

1. 인출 사이클 (Fetch Cycle)
    - CPU가 메모리에서 명령어를 가져오는 단계
    - 예: “명령어 하나를 실행하고 싶다” → 먼저 메모리에서 그 명령어를 CPU로 가져와야 함
2. 실행 사이클 (Execution Cycle)
    - 인출된 명령어를 CPU가 실제로 실행하는 단계
    - 즉, 연산 수행, 메모리 접근, 제어 흐름 변경 등 명령어의 목적을 수행

![image](https://github.com/user-attachments/assets/a5ddfcc3-a17f-44d1-842a-79ffa51e4597)


> CPU는 인출 사이클과 실행 사이클을 계속 반복하여 프로그램을 실행한다
> 

### 간접 사이클 (Indirect Cycle)

- 그런데 명령어 실행이 항상 단순하지는 않음. 모든 명령어가 인출, 바로 실행 가능한 것은 아님
- 예를 들어, 명령어의 오퍼랜드 필드에 메모리 주소가 들어 있는 경우, CPU는 그 주소에 한 번 더 메모리 접근을 해야 함
- 오퍼랜드가 실제 값이 아니라 주소인 경우, 해당 주소를 따라가 진짜 데이터를 메모리에서 한 번 더 가져와야 함
- 이 추가적인 메모리 접근 단계를 간접 사이클이라 부름

![image](https://github.com/user-attachments/assets/28c4571f-2713-4a04-bbc4-55905c2dd29c)


### 인터럽트 사이클

- 추가로 매우 중요한 사이클인 인터럽트 사이클(Interrupt Cycle)이 존재함
- 외부 장치나 예외 상황 등에서 발생하는 인터럽트를 처리하는 전용 사이클
- 인터럽트 사이클을 제대로 이해하려면 CPU 레지스터 구조에 대한 선행 이해가 필요함
